{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Load Vocabulary\n",
    "# ----------------------\n",
    "def load_vocab(model_name):\n",
    "    vocab_path = os.path.join(\"models\", model_name, \"vocab-dict.json\")\n",
    "    with open(vocab_path, 'r') as json_file:\n",
    "        vocab = json.load(json_file)\n",
    "    return vocab\n",
    "\n",
    "# ----------------------\n",
    "# Load Model\n",
    "# ----------------------\n",
    "def load_model(model_name, vocab):\n",
    "    def mlp(num_inputs, num_hiddens, flatten):\n",
    "        model = []\n",
    "        model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Linear(num_inputs, num_hiddens))\n",
    "        model.append(nn.ReLU())\n",
    "        if flatten:\n",
    "            model.append(nn.Flatten(start_dim=1))\n",
    "        model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Linear(num_hiddens, num_hiddens))\n",
    "        model.append(nn.ReLU())\n",
    "        if flatten:\n",
    "            model.append(nn.Flatten(start_dim=1))\n",
    "        return nn.Sequential(*model)\n",
    "    \n",
    "    class Attend(nn.Module):\n",
    "        def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "            super(Attend, self).__init__(**kwargs)\n",
    "            self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "        def forward(self, A, B):\n",
    "            f_A = self.f(A)\n",
    "            f_B = self.f(B)\n",
    "            e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n",
    "            beta = torch.bmm(F.softmax(e, dim=-1), B)\n",
    "            alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
    "            return beta, alpha\n",
    "        \n",
    "    class Compare(nn.Module):\n",
    "        def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "            super(Compare, self).__init__(**kwargs)\n",
    "            self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "        def forward(self, A, B, beta, alpha):\n",
    "            V_A = self.g(torch.cat([A, beta], dim=2))\n",
    "            V_B = self.g(torch.cat([B, alpha], dim=2))\n",
    "            return V_A, V_B\n",
    "\n",
    "    class Aggregate(nn.Module):\n",
    "        def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
    "            super(Aggregate, self).__init__(**kwargs)\n",
    "            self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
    "            self.linear = nn.Linear(num_hiddens, num_outputs)\n",
    "        def forward(self, V_A, V_B):\n",
    "            V_A = V_A.sum(dim=1)\n",
    "            V_B = V_B.sum(dim=1)\n",
    "            Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n",
    "            return Y_hat\n",
    "        \n",
    "    class DecomposableAttention(nn.Module):\n",
    "        def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\n",
    "                    num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
    "            super(DecomposableAttention, self).__init__(**kwargs)\n",
    "            self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "            self.attend = Attend(num_inputs_attend, num_hiddens)\n",
    "            self.compare = Compare(num_inputs_compare, num_hiddens)\n",
    "            self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
    "        def forward(self, X):\n",
    "            premises, hypotheses = X\n",
    "            A = self.embedding(premises)\n",
    "            B = self.embedding(hypotheses)\n",
    "            beta, alpha = self.attend(A, B)\n",
    "            V_A, V_B = self.compare(A, B, beta, alpha)\n",
    "            Y_hat = self.aggregate(V_A, V_B)\n",
    "            return Y_hat\n",
    "\n",
    "    model = DecomposableAttention(vocab, 100, 200)\n",
    "    model_path = os.path.join(\"models\", model_name, \"model-state.pt\")\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device('cpu')))\n",
    "    return model\n",
    "\n",
    "# ----------------------\n",
    "# Predict Function\n",
    "# ----------------------\n",
    "def predict_snli(model, vocab, premises, hypotheses):\n",
    "    premise = premises.strip().split()\n",
    "    hypothesis = hypotheses.strip().split()\n",
    "    premise_indices = [vocab.get(word, vocab[\"<pad>\"]) for word in premise]\n",
    "    hypothesis_indices = [vocab.get(word, vocab[\"<pad>\"]) for word in hypothesis]\n",
    "    model.eval()\n",
    "    premise_tensor = torch.tensor(premise_indices).unsqueeze(0)\n",
    "    hypothesis_tensor = torch.tensor(hypothesis_indices).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model((premise_tensor, hypothesis_tensor))\n",
    "        label_idx = torch.argmax(outputs, dim=1).item()\n",
    "    return \"entailment\" if label_idx == 0 else \"contradiction\" if label_idx == 1 else \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# User Interface\n",
    "# ----------------------\n",
    "def create_nli_interface(model_name=\"decomposable-attention\"):\n",
    "    vocab = load_vocab(model_name)\n",
    "    model = load_model(model_name, vocab)\n",
    "\n",
    "    title = widgets.Label(value=\"Natural Language Inference\")\n",
    "    premise_input = widgets.Textarea(description=\"Premise:\", placeholder=\"e.g. A soccer game with multiple males playing.\")\n",
    "    hypothesis_input = widgets.Textarea(description=\"Hypothesis:\", placeholder=\"e.g. A soccer game with multiple males playing.\")\n",
    "    output_area = widgets.Textarea(value=\"Result:\", layout=widgets.Layout(height='50px'), disabled=True)\n",
    "    infer_button = widgets.Button(description=\"Infer\")\n",
    "    \n",
    "    def on_infer_clicked(b):\n",
    "        premise = premise_input.value\n",
    "        hypothesis = hypothesis_input.value\n",
    "        if premise and hypothesis:\n",
    "            result = predict_snli(model, vocab, premise, hypothesis)\n",
    "            output_area.value = f\"Result: {result.capitalize()}\"\n",
    "        else:\n",
    "            output_area.value = \"Please enter some text for inference.\"\n",
    "    \n",
    "    infer_button.on_click(on_infer_clicked)\n",
    "    \n",
    "    display(widgets.VBox([title, premise_input, hypothesis_input, infer_button, output_area]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb8bc2880544a6593f41db75eb4d529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Natural Language Inference'), Textarea(value='', description='Premise:', placeholdâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_nli_interface(\"decomposable-attention\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
